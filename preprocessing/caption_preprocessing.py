import os
import json
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
import pickle

# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
train_videos_path = r"C:\Mennah\semster 8\deep learning\assigments\video captioning\main_project\dataset\train"
json_path = r"C:\Mennah\semster 8\deep learning\assigments\video captioning\main_project\captions_train.json"
features_path = r"C:\Mennah\semster 8\deep learning\assigments\video captioning\main_project\features"

# Ù‚Ø±Ø§Ø¡Ø© JSON
with open(json_path, 'r', encoding='utf-8') as f:
    captions_data = json.load(f)

print("âœ… Ø¹Ø¯Ø¯ Ø¹Ù†Ø§ØµØ± JSON:", len(captions_data))
print("ğŸ”¸ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:", type(captions_data))
print("ğŸ”¸ Ù…Ø«Ø§Ù„ Ø£ÙˆÙ„ Ø¹Ù†ØµØ±:", captions_data[0])

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {'video_id': [captions...]}
captions_dict = {}
for item in captions_data:
    video_id = item['id']
    captions = [cap.strip() for cap in item['caption']]
    captions_dict[video_id] = captions

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†Ø§Øª
all_captions = []
video_caption_pairs = []

missing_files = 0
for video_id, caps in captions_dict.items():
    video_id_clean = video_id.replace(".avi", "")  # Ø´ÙŠÙ„ .avi
    feature_file = os.path.join(features_path, video_id_clean + ".npy")
    if os.path.exists(feature_file):
        for cap in caps:
            caption = "<start> " + cap.lower() + " <end>"
            all_captions.append(caption)
            video_caption_pairs.append((feature_file, caption))
    else:
        missing_files += 1
        print(f"âŒ Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {feature_file}")

print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†Ø² Ø¨Ø¹Ø¯ Ø§Ù„ÙÙ„ØªØ±Ø©: {len(all_captions)}")
if len(all_captions) == 0:
    print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ø¨Ø´Ù†Ø§Øª ØµØ§Ù„Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙÙ„ØªØ±Ø©! ØªØ£ÙƒØ¯ÙŠ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª .npy ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­.")
    exit()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø±
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(all_captions)

print(f"âœ… Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {len(tokenizer.word_index)}")
print(f"ğŸ”¸ Ø£ÙˆÙ„ 10 ÙƒÙ„Ù…Ø§Øª: {list(tokenizer.word_index.items())[:10]}")

# Ø­ÙØ¸ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø±
tokenizer_dir = os.path.join("C:/Mennah/semster 8/deep learning/assigments/video captioning/main_project", "tokenizer")
os.makedirs(tokenizer_dir, exist_ok=True)

with open(os.path.join(tokenizer_dir, "tokenizer.pkl"), "wb") as f:
    pickle.dump(tokenizer, f)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†Ø§Øª Ù„Ø£Ø±Ù‚Ø§Ù…
sequences = tokenizer.texts_to_sequences([cap for _, cap in video_caption_pairs])
max_length = max(len(seq) for seq in sequences)
print(f"âœ… Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ÙƒØ§Ø¨Ø´Ù†: {max_length}")

# Ø­ÙØ¸ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰
with open(os.path.join(tokenizer_dir, "max_length.txt"), "w") as f:
    f.write(str(max_length))

# Ø­ÙØ¸ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„ÙƒØ§Ø¨Ø´Ù†
video_caption_pairs_array = np.array(video_caption_pairs, dtype=object)
np.save(os.path.join(tokenizer_dir, "video_caption_pairs.npy"), video_caption_pairs_array)

print(f"âœ… ØªÙ… Ø­ÙØ¸ tokenizer Ùˆ max_length Ùˆ video_caption_pairs âœ…")
print(f"ğŸ”¹ Ø¹Ø¯Ø¯ Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {missing_files}")
 